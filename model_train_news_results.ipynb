{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/changhorang/text_classification.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqADg3DgGHLK",
        "outputId": "d58a1b36-c9c1-436c-b351-84da039acf83"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'text_classification'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 50 (delta 20), reused 36 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (50/50), done.\n",
            "Checking out files: 100% (19/19), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IscJrkn9t5b3",
        "outputId": "86f0470c-3d67-4088-b492-15b1142a7e4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 32.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(r'text_classification')\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SlKt6SCKGTnv",
        "outputId": "dd252faf-286f-488a-ecd9-cd3f3e870840"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/text_classification'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main2.py --lr 1e-4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68A8XhHXvKUk",
        "outputId": "e878bbec-3e37-4a60-a7e8-16c2f06ceca3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "tcmalloc: large alloc 2250547200 bytes == 0x55609d7b0000 @  0x7f2e03344001 0x7f2dfeccb1af 0x7f2dfed27557 0x7f2dfed27d1b 0x7f2dfedc8333 0x5560930f611c 0x5560930f5ef0 0x55609316a123 0x5560930f77aa 0x5560931658f6 0x556093164a2e 0x556093164723 0x55609322e812 0x55609322eb8d 0x55609322ea36 0x556093206183 0x556093205e2c 0x7f2e0212cc87 0x556093205d0a\n",
            "tcmalloc: large alloc 1913880576 bytes == 0x55613968e000 @  0x7f2e03344001 0x7f2dfeccb1af 0x7f2dfed27557 0x7f2dfed27d1b 0x7f2dfedc8333 0x5560930f611c 0x5560930f5ef0 0x55609316a123 0x5560930f77aa 0x5560931658f6 0x556093164a2e 0x556093164723 0x55609322e812 0x55609322eb8d 0x55609322ea36 0x556093206183 0x556093205e2c 0x7f2e0212cc87 0x556093205d0a\n",
            "Tokenizing data...\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc \n",
            "----------------------------------------------------------------------\n",
            "   1    |   100   |   0.694538   |     -      |     -    \n",
            "   1    |   200   |   0.691755   |     -      |     -    \n",
            "   1    |   300   |   0.693834   |     -      |     -    \n",
            "   1    |   400   |   0.693226   |     -      |     -    \n",
            "   1    |   500   |   0.693799   |     -      |     -    \n",
            "   1    |   600   |   0.693989   |     -      |     -    \n",
            "   1    |   700   |   0.694009   |     -      |     -    \n",
            "   1    |   800   |   0.694628   |     -      |     -    \n",
            "   1    |   900   |   0.693130   |     -      |     -    \n",
            "   1    |  1000   |   0.693313   |     -      |     -    \n",
            "   1    |  1100   |   0.693277   |     -      |     -    \n",
            "   1    |  1200   |   0.693796   |     -      |     -    \n",
            "   1    |  1300   |   0.693750   |     -      |     -    \n",
            "   1    |  1400   |   0.693949   |     -      |     -    \n",
            "   1    |  1500   |   0.693622   |     -      |     -    \n",
            "   1    |  1600   |   0.693758   |     -      |     -    \n",
            "   1    |  1700   |   0.693571   |     -      |     -    \n",
            "   1    |  1800   |   0.692793   |     -      |     -    \n",
            "   1    |  1900   |   0.694100   |     -      |     -    \n",
            "   1    |  2000   |   0.693052   |     -      |     -    \n",
            "   1    |  2100   |   0.693988   |     -      |     -    \n",
            "   1    |  2200   |   0.692966   |     -      |     -    \n",
            "   1    |  2300   |   0.693584   |     -      |     -    \n",
            "   1    |  2400   |   0.694004   |     -      |     -    \n",
            "   1    |  2500   |   0.693330   |     -      |     -    \n",
            "   1    |  2600   |   0.693151   |     -      |     -    \n",
            "   1    |  2700   |   0.692807   |     -      |     -    \n",
            "   1    |  2800   |   0.693488   |     -      |     -    \n",
            "   1    |  2900   |   0.694107   |     -      |     -    \n",
            "   1    |  3000   |   0.693112   |     -      |     -    \n",
            "   1    |  3100   |   0.693271   |     -      |     -    \n",
            "   1    |  3200   |   0.693811   |     -      |     -    \n",
            "   1    |  3300   |   0.693680   |     -      |     -    \n",
            "   1    |  3400   |   0.693331   |     -      |     -    \n",
            "   1    |  3500   |   0.693276   |     -      |     -    \n",
            "   1    |  3532   |   0.693512   |     -      |     -    \n",
            "   1    |    -    |   0.693537   |  0.693157  |   49.97  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc \n",
            "----------------------------------------------------------------------\n",
            "   2    |   100   |   0.691939   |     -      |     -    \n",
            "   2    |   200   |   0.694514   |     -      |     -    \n",
            "   2    |   300   |   0.693719   |     -      |     -    \n",
            "   2    |   400   |   0.693540   |     -      |     -    \n",
            "   2    |   500   |   0.693695   |     -      |     -    \n",
            "   2    |   600   |   0.693795   |     -      |     -    \n",
            "   2    |   700   |   0.693231   |     -      |     -    \n",
            "   2    |   800   |   0.693442   |     -      |     -    \n",
            "   2    |   900   |   0.693684   |     -      |     -    \n",
            "   2    |  1000   |   0.693240   |     -      |     -    \n",
            "   2    |  1100   |   0.693318   |     -      |     -    \n",
            "   2    |  1200   |   0.693498   |     -      |     -    \n",
            "   2    |  1300   |   0.693061   |     -      |     -    \n",
            "   2    |  1400   |   0.693580   |     -      |     -    \n",
            "   2    |  1500   |   0.693585   |     -      |     -    \n",
            "   2    |  1600   |   0.693674   |     -      |     -    \n",
            "   2    |  1700   |   0.693424   |     -      |     -    \n",
            "   2    |  1800   |   0.693361   |     -      |     -    \n",
            "   2    |  1900   |   0.693545   |     -      |     -    \n",
            "   2    |  2000   |   0.693352   |     -      |     -    \n",
            "   2    |  2100   |   0.693311   |     -      |     -    \n",
            "   2    |  2200   |   0.693746   |     -      |     -    \n",
            "   2    |  2300   |   0.693046   |     -      |     -    \n",
            "   2    |  2400   |   0.692941   |     -      |     -    \n",
            "   2    |  2500   |   0.693156   |     -      |     -    \n",
            "   2    |  2600   |   0.693550   |     -      |     -    \n",
            "   2    |  2700   |   0.693112   |     -      |     -    \n",
            "   2    |  2800   |   0.693395   |     -      |     -    \n",
            "   2    |  2900   |   0.693255   |     -      |     -    \n",
            "   2    |  3000   |   0.693548   |     -      |     -    \n",
            "   2    |  3100   |   0.693419   |     -      |     -    \n",
            "   2    |  3200   |   0.694301   |     -      |     -    \n",
            "   2    |  3300   |   0.693512   |     -      |     -    \n",
            "   2    |  3400   |   0.693422   |     -      |     -    \n",
            "   2    |  3500   |   0.693405   |     -      |     -    \n",
            "   2    |  3532   |   0.693453   |     -      |     -    \n",
            "   2    |    -    |   0.693437   |  0.693151  |   50.03  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    }
  ]
}